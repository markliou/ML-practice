{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf833a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 15:14:46.072451: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742051686.085915   32036 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742051686.089943   32036 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742051686.101860   32036 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742051686.101873   32036 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742051686.101875   32036 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742051686.101877   32036 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-15 15:14:46.105618: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "import jax\n",
    "import flax.nnx\n",
    "import optax\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b7399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(flax.nnx.Module):\n",
    "    def __init__(self, \n",
    "                 regularizer = 1e-4,\n",
    "                 *args, \n",
    "                 **kwargs):\n",
    "      \n",
    "        self.rngs = flax.nnx.Rngs(3)\n",
    "        self.regularizer = regularizer\n",
    "        self.loss = 0.\n",
    "        self.layers = []\n",
    "        \n",
    "        self.conv1 = flax.nnx.Conv(1, 32, (3, 3), strides=2, padding='SAME', rngs=self.rngs) # [N, 16, 16,32]\n",
    "        self.conv2 = flax.nnx.Conv(32, 64, (3, 3), strides=2, padding='SAME', rngs=self.rngs) # [N, 8, 8, 64]\n",
    "        self.conv3 = flax.nnx.Conv(64, 128, (3, 3), strides=2, padding='SAME', rngs=self.rngs) # [N, 4, 4, 128]\n",
    "        self.conv4 = flax.nnx.Conv(128, 256, (3, 3), strides=2, padding='SAME', rngs=self.rngs) # [N, 2, 2, 256]\n",
    "        self.conv5 = flax.nnx.Conv(256, 10, (3, 3), strides=2, padding='SAME', rngs=self.rngs) # [N, 1, 1, 10]\n",
    "    \n",
    "        self.layers = [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5]\n",
    "    \n",
    "    @flax.nnx.jit\n",
    "    def __call__(self, x):\n",
    "        conv1 = flax.nnx.relu(self.conv1(x))\n",
    "        conv2 = flax.nnx.relu(self.conv2(conv1))\n",
    "        conv3 = flax.nnx.relu(self.conv3(conv2))\n",
    "        conv4 = flax.nnx.relu(self.conv4(conv3))\n",
    "        out = self.conv5(conv4)\n",
    "        \n",
    "        return out.reshape(-1, 10)\n",
    "    \n",
    "    # addtional loss\n",
    "    def kernel_bias_L2regularization(self):\n",
    "        loss = 0 # 不能用self.loss，因為tree_util的結果不給儲存\n",
    "        for layer in self.layers:\n",
    "            # weights regularization\n",
    "            loss += (jax.tree_util.tree_leaves(layer.kernel)[0].sum()) ** 2 \n",
    "            loss += (jax.tree_util.tree_leaves(layer.bias)[0].sum()) ** 2 \n",
    "        return loss * self.regularizer\n",
    "    \n",
    "model = cnn()\n",
    "# flax.nnx.display(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440133d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 15:14:50.813616: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1742051690.813749   32036 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5489 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "# Define a function to normalize the images\n",
    "def normalize_img(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "# Prepare the training dataset\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE).repeat()\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(32, drop_remainder=True)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Prepare the test dataset\n",
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE).repeat()\n",
    "ds_test = ds_test.batch(32, drop_remainder=True)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# initialize training and test dataset iterator\n",
    "tr_iter = iter(ds_train)\n",
    "ts_iter = iter(ds_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48316747",
   "metadata": {},
   "outputs": [],
   "source": [
    "###! 不知道為什麼，loss function最前面一定要放model，不然會報錯。例如放(images,label,model)，把model放最後一個就會出錯\n",
    "def loss_fn(model, x, y):\n",
    "    y_hat_logits = model(jnp.array(x))\n",
    "    ce = optax.losses.softmax_cross_entropy_with_integer_labels(logits=y_hat_logits, labels=jnp.array(y)).mean()\n",
    "    reg = model.kernel_bias_L2regularization()\n",
    "    return ce + reg, y_hat_logits\n",
    "\n",
    "# @flax.nnx.jit\n",
    "def update_weights(model, opt, tr_iter, innerSteps=100):\n",
    "    for innerStep in range(innerSteps):\n",
    "        pics, labels = next(tr_iter)\n",
    "        # 如果loss function有回傳loss以外的東西就要把has_aux打開，\n",
    "        # value_and_grad的回傳規則是 [(loss_fn Arg1, loss_fn Arg2, ...), gradient]\n",
    "        grad_fn = flax.nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "        (loss, logits), grads = grad_fn(model, pics, labels)\n",
    "        opt.update(grads)\n",
    "    return loss, logits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e64a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 15:14:51.347629: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss:1.5701472759246826 val acc:0.8125\n",
      "step 100 loss:0.6020104289054871 val acc:0.875\n",
      "step 200 loss:0.3363880217075348 val acc:0.90625\n",
      "step 300 loss:0.3578474819660187 val acc:0.875\n",
      "step 400 loss:0.5462193489074707 val acc:0.90625\n",
      "step 500 loss:0.09150280803442001 val acc:1.0\n",
      "step 600 loss:0.3464528024196625 val acc:0.90625\n",
      "step 700 loss:0.18859736621379852 val acc:0.9375\n",
      "step 800 loss:0.0718141421675682 val acc:0.96875\n",
      "step 900 loss:0.1552731990814209 val acc:0.96875\n",
      "step 1000 loss:0.17164109647274017 val acc:0.9375\n",
      "step 1100 loss:0.3645014464855194 val acc:0.90625\n",
      "step 1200 loss:0.06559634208679199 val acc:1.0\n",
      "step 1300 loss:0.15293672680854797 val acc:0.9375\n",
      "step 1400 loss:0.07625730335712433 val acc:1.0\n",
      "step 1500 loss:0.21861226856708527 val acc:0.90625\n",
      "step 1600 loss:0.05607498809695244 val acc:0.96875\n",
      "step 1700 loss:0.05368279665708542 val acc:1.0\n",
      "step 1800 loss:0.21398814022541046 val acc:0.9375\n",
      "step 1900 loss:0.06544697284698486 val acc:0.96875\n",
      "step 2000 loss:0.1340591013431549 val acc:0.96875\n"
     ]
    }
   ],
   "source": [
    "learningRate = 1e-4\n",
    "trainingStep = 50\n",
    "opt = flax.nnx.Optimizer(model, optax.adamw(learningRate))\n",
    "\n",
    "# 如果loss function有回傳loss以外的東西就要把has_aux打開，\n",
    "# value_and_grad的回傳規則是 [(loss_fn Arg1, loss_fn Arg2, ...), gradient]\n",
    "grad_fn = flax.nnx.value_and_grad(loss_fn, has_aux=False)\n",
    "\n",
    "for step in range(trainingStep):\n",
    "    # optimizing model\n",
    "    loss, logits = update_weights(model, opt, tr_iter)\n",
    "    # using test dataset to validate the model with accuracy\n",
    "    pics, labels = next(ts_iter)\n",
    "    loss, logits = loss_fn(model, pics, labels)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, axis=1) == jnp.array(labels))\n",
    "    \n",
    "    print(\"step {} loss:{} val acc:{}\".format(step * 100, loss, accuracy))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
