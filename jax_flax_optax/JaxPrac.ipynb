{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf833a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 20:04:41.276239: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-14 20:04:41.291087: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741982681.307821    9323 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741982681.313088    9323 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1741982681.325136    9323 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1741982681.325168    9323 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1741982681.325169    9323 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1741982681.325170    9323 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-14 20:04:41.329835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "import jax\n",
    "import flax.nnx\n",
    "import optax\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b7399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(flax.nnx.Module):\n",
    "    def __init__(self, \n",
    "                 regularizer = 1e-4,\n",
    "                 *args, \n",
    "                 **kwargs):\n",
    "      \n",
    "        self.rngs = flax.nnx.Rngs(3)\n",
    "        self.regularizer = regularizer\n",
    "        self.loss = 0.\n",
    "        self.layers = []\n",
    "        \n",
    "        self.conv1 = flax.nnx.Conv(1, 32, (3, 3), strides=2, padding='SAME', rngs=self.rngs) # [N, 16, 16,32]\n",
    "        self.conv2 = flax.nnx.Conv(32, 64, (3, 3), strides=2, padding='SAME', rngs=self.rngs) # [N, 8, 8, 64]\n",
    "        self.conv3 = flax.nnx.Conv(64, 128, (3, 3), strides=2, padding='SAME', rngs=self.rngs) # [N, 4, 4, 128]\n",
    "        self.conv4 = flax.nnx.Conv(128, 256, (3, 3), strides=2, padding='SAME', rngs=self.rngs) # [N, 2, 2, 256]\n",
    "        self.conv5 = flax.nnx.Conv(256, 10, (3, 3), strides=2, padding='SAME', rngs=self.rngs) # [N, 1, 1, 10]\n",
    "    \n",
    "        self.layers = [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5]\n",
    "    \n",
    "    @flax.nnx.jit\n",
    "    def __call__(self, x):\n",
    "        conv1 = flax.nnx.relu(self.conv1(x))\n",
    "        conv2 = flax.nnx.relu(self.conv2(conv1))\n",
    "        conv3 = flax.nnx.relu(self.conv3(conv2))\n",
    "        conv4 = flax.nnx.relu(self.conv4(conv3))\n",
    "        out = self.conv5(conv4)\n",
    "        \n",
    "        return out.reshape(-1, 10)\n",
    "    \n",
    "    # addtional loss\n",
    "    def kernel_bias_L2regularization(self):\n",
    "        self.loss = 0\n",
    "        for layer in self.layers:\n",
    "            # weights regularization\n",
    "            self.loss += (layer.kernel.value.sum() + layer.bias.value.sum()) ** 2 * self.regularizer\n",
    "        return self.loss\n",
    "    \n",
    "model = cnn()\n",
    "# flax.nnx.display(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440133d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 20:04:51.799162: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1741982691.799252    9323 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1747 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "# Define a function to normalize the images\n",
    "def normalize_img(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "# Prepare the training dataset\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE).repeat()\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(32, drop_remainder=True)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Prepare the test dataset\n",
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE).repeat()\n",
    "ds_test = ds_test.batch(32, drop_remainder=True)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# initialize training and test dataset iterator\n",
    "tr_iter = iter(ds_train)\n",
    "ts_iter = iter(ds_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48316747",
   "metadata": {},
   "outputs": [],
   "source": [
    "###! 不知道為什麼，loss function最前面一定要放model，不然會報錯。例如放(images,label,model)，把model放最後一個就會出錯\n",
    "def loss_fn(model, x, y):\n",
    "    y_hat_logits = model(jnp.array(x))\n",
    "    ce = optax.losses.softmax_cross_entropy_with_integer_labels(logits=y_hat_logits, labels=jnp.array(y)).mean()\n",
    "    reg = model.kernel_bias_L2regularization()\n",
    "    return ce + reg, y_hat_logits\n",
    "\n",
    "# @flax.nnx.jit\n",
    "def update_weights(model, opt, tr_iter, innerSteps=100):\n",
    "    for innerStep in range(innerSteps):\n",
    "        pics, labels = next(tr_iter)\n",
    "        # 如果loss function有回傳loss以外的東西就要把has_aux打開，\n",
    "        # value_and_grad的回傳規則是 [(loss_fn Arg1, loss_fn Arg2, ...), gradient]\n",
    "        grad_fn = flax.nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "        (loss, logits), grads = grad_fn(model, pics, labels)\n",
    "        opt.update(grads)\n",
    "    return loss, logits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13e64a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 20:05:14.652803: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arrays leaves are not supported, at 'loss': Traced<ShapedArray(float32[])>with<JVPTrace> with\n  primal = Array(0.0186766, dtype=float32)\n  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace> with\n    pval = (ShapedArray(float32[]), None)\n    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7f42c03c0c10>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace>, Traced<ShapedArray(float32[]):JaxprTrace>), out_tracer_refs=[<weakref at 0x7f42c03d7bf0; to 'JaxprTracer' at 0x7f42c03d7b50>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[]\u001b[39m b\u001b[35m:f32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\u001b[39m\u001b[22m\u001b[22m c\u001b[35m:f32[]\u001b[39m = add a b \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'add', 'keep_unused': False, 'inline': True, 'compiler_options_kvs': ()}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7f42c03e2860>, ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=True, cur_abstract_mesh=AbstractMesh((), axis_types={}), xla_metadata=None))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m grad_fn = flax.nnx.value_and_grad(loss_fn, has_aux=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trainingStep):\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# optimizing model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     loss, logits = \u001b[43mupdate_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# using test dataset to validate the model with accuracy\u001b[39;00m\n\u001b[32m     13\u001b[39m     pics, labels = \u001b[38;5;28mnext\u001b[39m(ts_iter)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mupdate_weights\u001b[39m\u001b[34m(model, opt, tr_iter, innerSteps)\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# 如果loss function有回傳loss以外的東西就要把has_aux打開，\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# value_and_grad的回傳規則是 [(loss_fn Arg1, loss_fn Arg2, ...), gradient]\u001b[39;00m\n\u001b[32m     14\u001b[39m     grad_fn = flax.nnx.value_and_grad(loss_fn, has_aux=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     (loss, logits), grads = \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     opt.update(grads)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss, logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/flax/nnx/graph.py:1832\u001b[39m, in \u001b[36mUpdateContextManager.__call__.<locals>.update_context_manager_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1829\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(f)\n\u001b[32m   1830\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_context_manager_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m   1831\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1832\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/flax/nnx/transforms/autodiff.py:165\u001b[39m, in \u001b[36m_grad_general.<locals>.grad_wrapper\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    152\u001b[39m pure_args = extract.to_tree(\n\u001b[32m    153\u001b[39m   args, prefix=arg_filters, split_fn=_grad_split_fn, ctxtag=\u001b[33m'\u001b[39m\u001b[33mgrad\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    154\u001b[39m )\n\u001b[32m    156\u001b[39m gradded_fn = transform(\n\u001b[32m    157\u001b[39m   GradFn(f, has_aux, nondiff_states),\n\u001b[32m    158\u001b[39m   argnums=jax_argnums,\n\u001b[32m   (...)\u001b[39m\u001b[32m    162\u001b[39m   reduce_axes=reduce_axes,\n\u001b[32m    163\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m fn_out = \u001b[43mgradded_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpure_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_grads\u001b[39m(grads):\n\u001b[32m    168\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m jax.tree.map(\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: x.state \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, extract.NodeStates) \u001b[38;5;28;01melse\u001b[39;00m x,\n\u001b[32m    170\u001b[39m     grads,\n\u001b[32m    171\u001b[39m     is_leaf=\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28misinstance\u001b[39m(x, extract.NodeStates),\n\u001b[32m    172\u001b[39m   )\n",
      "    \u001b[31m[... skipping hidden 16 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/flax/nnx/transforms/autodiff.py:91\u001b[39m, in \u001b[36mGradFn.__call__\u001b[39m\u001b[34m(self, *pure_args)\u001b[39m\n\u001b[32m     88\u001b[39m out = \u001b[38;5;28mself\u001b[39m.f(*args)\n\u001b[32m     90\u001b[39m args_out = extract.clear_non_graph_nodes(args)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m pure_args_out, pure_out = \u001b[43mextract\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctxtag\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgrad\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_aux:\n\u001b[32m     94\u001b[39m   loss, pure_aux = pure_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/flax/nnx/extract.py:280\u001b[39m, in \u001b[36mto_tree\u001b[39m\u001b[34m(tree, prefix, split_fn, map_non_graph_nodes, ctxtag, check_aliasing)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m Missing \u001b[38;5;129;01mor\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    278\u001b[39m   \u001b[38;5;66;03m# fast path, no need for prefix broadcasting or consistent aliasing checks\u001b[39;00m\n\u001b[32m    279\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m graph.split_context(ctxtag) \u001b[38;5;28;01mas\u001b[39;00m split_ctx:\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_non_graph_nodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_graph_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    286\u001b[39m leaf_prefixes = broadcast_prefix(\n\u001b[32m    287\u001b[39m   prefix,\n\u001b[32m    288\u001b[39m   tree,\n\u001b[32m    289\u001b[39m   prefix_is_leaf=\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    290\u001b[39m )\n\u001b[32m    291\u001b[39m leaf_keys, treedef = jax.tree_util.tree_flatten_with_path(tree)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/jax/_src/tree.py:155\u001b[39m, in \u001b[36mmap\u001b[39m\u001b[34m(f, tree, is_leaf, *rest)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap\u001b[39m(f: Callable[..., Any],\n\u001b[32m    116\u001b[39m         tree: Any,\n\u001b[32m    117\u001b[39m         *rest: Any,\n\u001b[32m    118\u001b[39m         is_leaf: Callable[[Any], \u001b[38;5;28mbool\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> Any:\n\u001b[32m    119\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Maps a multi-input function over pytree args to produce a new pytree.\u001b[39;00m\n\u001b[32m    120\u001b[39m \n\u001b[32m    121\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m \u001b[33;03m    - :func:`jax.tree.reduce`\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_util\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/flax/nnx/extract.py:281\u001b[39m, in \u001b[36mto_tree.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m Missing \u001b[38;5;129;01mor\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    278\u001b[39m   \u001b[38;5;66;03m# fast path, no need for prefix broadcasting or consistent aliasing checks\u001b[39;00m\n\u001b[32m    279\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m graph.split_context(ctxtag) \u001b[38;5;28;01mas\u001b[39;00m split_ctx:\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m jax.tree.map(\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m       \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43msplit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m       \u001b[38;5;28;01mif\u001b[39;00m map_non_graph_nodes \u001b[38;5;129;01mor\u001b[39;00m graph.is_graph_node(x)\n\u001b[32m    283\u001b[39m       \u001b[38;5;28;01melse\u001b[39;00m x,\n\u001b[32m    284\u001b[39m       tree,\n\u001b[32m    285\u001b[39m     )\n\u001b[32m    286\u001b[39m leaf_prefixes = broadcast_prefix(\n\u001b[32m    287\u001b[39m   prefix,\n\u001b[32m    288\u001b[39m   tree,\n\u001b[32m    289\u001b[39m   prefix_is_leaf=\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    290\u001b[39m )\n\u001b[32m    291\u001b[39m leaf_keys, treedef = jax.tree_util.tree_flatten_with_path(tree)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/flax/nnx/extract.py:262\u001b[39m, in \u001b[36mdefault_split_fn\u001b[39m\u001b[34m(ctx, path, prefix, leaf)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_split_fn\u001b[39m(\n\u001b[32m    260\u001b[39m   ctx: graph.SplitContext, path: KeyPath, prefix: Prefix, leaf: Leaf\n\u001b[32m    261\u001b[39m ) -> tp.Any:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m NodeStates.from_split(*\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleaf\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/flax/nnx/graph.py:1309\u001b[39m, in \u001b[36mSplitContext.split\u001b[39m\u001b[34m(self, node, *filters)\u001b[39m\n\u001b[32m   1303\u001b[39m ctx = (\n\u001b[32m   1304\u001b[39m   current_update_context(\u001b[38;5;28mself\u001b[39m.ctxtag) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctxtag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1305\u001b[39m )\n\u001b[32m   1306\u001b[39m inner_ref_outer_index = (\n\u001b[32m   1307\u001b[39m   ctx.inner_ref_outer_index \u001b[38;5;28;01mif\u001b[39;00m ctx \u001b[38;5;129;01mand\u001b[39;00m ctx.inner_ref_outer_index \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1308\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1309\u001b[39m graphdef, flat_state = \u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1310\u001b[39m \u001b[43m  \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mref_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_outer_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_ref_outer_index\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m flat_states = _split_state(flat_state, filters)\n\u001b[32m   1313\u001b[39m states = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m   1314\u001b[39m   statelib.from_flat_state(flat_state) \u001b[38;5;28;01mfor\u001b[39;00m flat_state \u001b[38;5;129;01min\u001b[39;00m flat_states\n\u001b[32m   1315\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/flax/nnx/graph.py:502\u001b[39m, in \u001b[36mflatten\u001b[39m\u001b[34m(node, with_paths, return_variables, ref_index, ref_outer_index)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    501\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUnsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(node)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, this is a bug.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m graphdef = \u001b[43m_graph_flatten\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m  \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m  \u001b[49m\u001b[43mnode_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m  \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m  \u001b[49m\u001b[43mref_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m  \u001b[49m\u001b[43mref_outer_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m  \u001b[49m\u001b[43mleaves\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m  \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m  \u001b[49m\u001b[43mreturn_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m paths \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    514\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m graphdef, FlatState.from_sorted_keys_values(\u001b[38;5;28mtuple\u001b[39m(paths), leaves)  \u001b[38;5;66;03m# type: ignore[return-value]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/flax/nnx/graph.py:591\u001b[39m, in \u001b[36m_graph_flatten\u001b[39m\u001b[34m(node, node_impl, path, ref_index, ref_outer_index, leaves, paths, return_variables)\u001b[39m\n\u001b[32m    589\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    590\u001b[39m   path_str = \u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m.join(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, path))\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    592\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mArrays leaves are not supported, at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_str\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    593\u001b[39m   )\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    595\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mArrays leaves are not supported, found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Arrays leaves are not supported, at 'loss': Traced<ShapedArray(float32[])>with<JVPTrace> with\n  primal = Array(0.0186766, dtype=float32)\n  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace> with\n    pval = (ShapedArray(float32[]), None)\n    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7f42c03c0c10>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace>, Traced<ShapedArray(float32[]):JaxprTrace>), out_tracer_refs=[<weakref at 0x7f42c03d7bf0; to 'JaxprTracer' at 0x7f42c03d7b50>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[]\u001b[39m b\u001b[35m:f32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\u001b[39m\u001b[22m\u001b[22m c\u001b[35m:f32[]\u001b[39m = add a b \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'add', 'keep_unused': False, 'inline': True, 'compiler_options_kvs': ()}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7f42c03e2860>, ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=True, cur_abstract_mesh=AbstractMesh((), axis_types={}), xla_metadata=None))"
     ]
    }
   ],
   "source": [
    "learningRate = 1e-4\n",
    "trainingStep = 10\n",
    "opt = flax.nnx.Optimizer(model, optax.adamw(learningRate))\n",
    "\n",
    "# 如果loss function有回傳loss以外的東西就要把has_aux打開，\n",
    "# value_and_grad的回傳規則是 [(loss_fn Arg1, loss_fn Arg2, ...), gradient]\n",
    "grad_fn = flax.nnx.value_and_grad(loss_fn, has_aux=False)\n",
    "\n",
    "for step in range(trainingStep):\n",
    "    # optimizing model\n",
    "    loss, logits = update_weights(model, opt, tr_iter)\n",
    "    # using test dataset to validate the model with accuracy\n",
    "    pics, labels = next(ts_iter)\n",
    "    loss, logits = loss_fn(model, pics, labels)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, axis=1) == jnp.array(labels))\n",
    "    \n",
    "    print(\"step {} loss:{} val acc:{}\".format(step * 100, loss, accuracy))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
