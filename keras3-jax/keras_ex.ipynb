{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 07:44:34.398990: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-05 07:44:34.399035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-05 07:44:34.400140: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# This guide can only be run with the jax backend.\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import jax\n",
    "\n",
    "# We import TF so we can use tf.data.\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "    x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    x2 = keras.layers.Dense(64, activation=\"relu\")(x1)\n",
    "    outputs = keras.layers.Dense(10, name=\"predictions\")(x2)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 32\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784)).astype(\"float32\")\n",
    "x_test = np.reshape(x_test, (-1, 784)).astype(\"float32\")\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "# Reserve 10,000 samples for validation.\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=64).batch(batch_size)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_and_updates(trainable_variables, non_trainable_variables, x, y):\n",
    "    y_pred, non_trainable_variables = model.stateless_call(\n",
    "        trainable_variables, non_trainable_variables, x\n",
    "    )\n",
    "    print(y_pred)\n",
    "    loss = loss_fn(y, y_pred)\n",
    "    return loss, non_trainable_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, data):\n",
    "    trainable_variables, non_trainable_variables, optimizer_variables = state\n",
    "    x, y = data\n",
    "    (loss, non_trainable_variables), grads = grad_fn(\n",
    "        trainable_variables, non_trainable_variables, x, y\n",
    "    )\n",
    "    trainable_variables, optimizer_variables = optimizer.stateless_apply(\n",
    "        grads, trainable_variables, optimizer_variables\n",
    "    )\n",
    "    # Return updated state\n",
    "    return loss, (\n",
    "        trainable_variables,\n",
    "        non_trainable_variables,\n",
    "        optimizer_variables,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ConcreteArray([[  31.524109    135.56967      -1.3483505    13.247395    -21.111664\n",
      "    13.21363     -17.077835   -193.62439     -23.242966     49.782784  ]\n",
      " [ 120.16257     129.38353     174.23958     -71.56448    -177.88203\n",
      "   115.17963      60.960014   -133.63116     -58.19822      14.002335  ]\n",
      " [  30.426115     -5.1504784   -47.540955      0.6190672   -82.51708\n",
      "    22.109928    -11.260504      3.5780945    34.494873     17.191149  ]\n",
      " [  69.17001     -33.76323     -86.93546     -41.494022    -42.44924\n",
      "    34.626938    -47.154015    -61.131073    -10.318655     -4.7830553 ]\n",
      " [ 102.8268       28.495008    -67.533554    -54.241997    -59.45101\n",
      "    44.710487     20.848019    -50.22388     -82.16635      22.092155  ]\n",
      " [  13.620922    -13.641014   -142.45761     -66.45045      85.74654\n",
      "    35.61003    -107.85919    -145.10628     -78.96484      54.67764   ]\n",
      " [  51.729397     30.57917      24.945047    -35.59692    -148.33936\n",
      "    23.759144     68.97758     -28.783669    -88.26081      30.452114  ]\n",
      " [ 106.641266    113.581505     10.42519    -117.85018     -86.67053\n",
      "    49.882004    -18.139503   -193.28436     -95.51611     122.238174  ]\n",
      " [  66.07152      24.708927    -10.380026     25.89611    -112.12999\n",
      "    46.270523     32.641396     16.966246     -8.652697    -17.377666  ]\n",
      " [  22.741755     91.68441    -100.41368     -30.04673      34.33294\n",
      "    -1.3838611   -54.33109    -118.06445     -85.80609      90.26596   ]\n",
      " [  78.91527      51.00483      55.50154     -57.720634   -120.78711\n",
      "   106.042564     39.463547    -69.38458     -36.79931      15.835715  ]\n",
      " [  98.68685     -52.886295   -148.71753     -45.304985     47.278175\n",
      "     6.6319504   -56.4639     -102.61545       9.5802555    43.071106  ]\n",
      " [  74.27302      14.034821      5.431244     54.042343   -101.5483\n",
      "    68.47028      66.34796       2.3106537    30.521849    -37.889626  ]\n",
      " [  95.22371     197.10358      21.453423     33.138184    -91.3965\n",
      "    -1.6697006    48.226017   -108.11131      22.818317     77.27644   ]\n",
      " [  46.209396     17.735714    -38.143787     22.876942    -73.498\n",
      "    26.69917      42.845165     -4.606655    -11.495332      5.642314  ]\n",
      " [  32.32731     141.53775     -64.23381       7.97164     -39.19815\n",
      "    61.637524     40.01863    -147.85638     -22.069044     51.819225  ]\n",
      " [ 107.195694     59.10534      55.822372     -3.1747923   -85.96738\n",
      "    23.873898    -41.439056   -111.43588     -27.494167     56.691196  ]\n",
      " [  14.858396    -15.746067     -1.6181183   -81.50989       1.7595758\n",
      "    86.419464   -118.613365   -119.786575      8.458973    -54.489952  ]\n",
      " [  68.89317      -5.097273      8.811802     -3.2355022  -113.75881\n",
      "    28.01316      12.196131    -29.831564    -33.80815       0.34610748]\n",
      " [  65.13188      39.239567    -52.829044    -17.664284    -15.570488\n",
      "    57.712303    -11.356604   -117.48575     -37.7851       58.742435  ]\n",
      " [ 179.53789     152.9896      -41.536907   -173.8988     -141.38182\n",
      "   115.82469      28.771904      0.514061    -44.517258    152.46188   ]\n",
      " [ 121.26493     150.04068     116.04086     -26.340302   -168.84164\n",
      "   183.18245      21.2554      -83.811584    -62.013866    -21.024973  ]\n",
      " [  61.2767       28.859652    -44.503212    -25.751724    -22.92778\n",
      "    63.271168    -67.61974     -83.858345      7.4037266    71.079605  ]\n",
      " [  61.93302     -21.063799    -72.886635    -46.461628    -52.303394\n",
      "    38.491573    -57.776398    -65.44181     -29.099531     11.231719  ]\n",
      " [  17.781132     16.539177     47.806786    -18.723955    -49.761444\n",
      "   136.31174      13.5568695  -140.49199      -4.4914093     0.9842956 ]\n",
      " [  41.955788     21.666855     59.43221     -76.511826    -72.510925\n",
      "    61.12279    -117.44246    -110.03589     -39.008205     18.708435  ]\n",
      " [  37.569393     39.94857      22.186924     25.231594    -11.303387\n",
      "    45.427322    -11.447958     29.542168     26.687408    -41.071907  ]\n",
      " [  79.909355    126.6944       41.90334     -25.85498    -133.41057\n",
      "    32.73693      -5.789444    -98.535034    -37.37251     -26.293476  ]\n",
      " [ 120.27393      39.122284    137.96712      50.609413   -131.61176\n",
      "    12.026989     78.23692    -111.75758     -32.443687    -76.357925  ]\n",
      " [  18.014309    -23.185825     -4.6567764    35.40206       1.6618366\n",
      "    77.98479     -83.86986     -78.49914      28.019001     -9.957073  ]\n",
      " [ -24.437302     21.311459    -61.2084      -63.12914      35.698868\n",
      "   100.49983     -10.440374      2.2848778   -19.700405     30.748013  ]\n",
      " [   4.1715126    17.476788     12.912262    -28.6914      -32.10283\n",
      "    99.01076    -148.20798    -116.05668     -19.984215    -30.610023  ]], dtype=float32)>with<JVPTrace(level=2/0)> with\n",
      "  primal = Array([[  31.524109  ,  135.56967   ,   -1.3483505 ,   13.247395  ,\n",
      "         -21.111664  ,   13.21363   ,  -17.077835  , -193.62439   ,\n",
      "         -23.242966  ,   49.782784  ],\n",
      "       [ 120.16257   ,  129.38353   ,  174.23958   ,  -71.56448   ,\n",
      "        -177.88203   ,  115.17963   ,   60.960014  , -133.63116   ,\n",
      "         -58.19822   ,   14.002335  ],\n",
      "       [  30.426115  ,   -5.1504784 ,  -47.540955  ,    0.6190672 ,\n",
      "         -82.51708   ,   22.109928  ,  -11.260504  ,    3.5780945 ,\n",
      "          34.494873  ,   17.191149  ],\n",
      "       [  69.17001   ,  -33.76323   ,  -86.93546   ,  -41.494022  ,\n",
      "         -42.44924   ,   34.626938  ,  -47.154015  ,  -61.131073  ,\n",
      "         -10.318655  ,   -4.7830553 ],\n",
      "       [ 102.8268    ,   28.495008  ,  -67.533554  ,  -54.241997  ,\n",
      "         -59.45101   ,   44.710487  ,   20.848019  ,  -50.22388   ,\n",
      "         -82.16635   ,   22.092155  ],\n",
      "       [  13.620922  ,  -13.641014  , -142.45761   ,  -66.45045   ,\n",
      "          85.74654   ,   35.61003   , -107.85919   , -145.10628   ,\n",
      "         -78.96484   ,   54.67764   ],\n",
      "       [  51.729397  ,   30.57917   ,   24.945047  ,  -35.59692   ,\n",
      "        -148.33936   ,   23.759144  ,   68.97758   ,  -28.783669  ,\n",
      "         -88.26081   ,   30.452114  ],\n",
      "       [ 106.641266  ,  113.581505  ,   10.42519   , -117.85018   ,\n",
      "         -86.67053   ,   49.882004  ,  -18.139503  , -193.28436   ,\n",
      "         -95.51611   ,  122.238174  ],\n",
      "       [  66.07152   ,   24.708927  ,  -10.380026  ,   25.89611   ,\n",
      "        -112.12999   ,   46.270523  ,   32.641396  ,   16.966246  ,\n",
      "          -8.652697  ,  -17.377666  ],\n",
      "       [  22.741755  ,   91.68441   , -100.41368   ,  -30.04673   ,\n",
      "          34.33294   ,   -1.3838611 ,  -54.33109   , -118.06445   ,\n",
      "         -85.80609   ,   90.26596   ],\n",
      "       [  78.91527   ,   51.00483   ,   55.50154   ,  -57.720634  ,\n",
      "        -120.78711   ,  106.042564  ,   39.463547  ,  -69.38458   ,\n",
      "         -36.79931   ,   15.835715  ],\n",
      "       [  98.68685   ,  -52.886295  , -148.71753   ,  -45.304985  ,\n",
      "          47.278175  ,    6.6319504 ,  -56.4639    , -102.61545   ,\n",
      "           9.5802555 ,   43.071106  ],\n",
      "       [  74.27302   ,   14.034821  ,    5.431244  ,   54.042343  ,\n",
      "        -101.5483    ,   68.47028   ,   66.34796   ,    2.3106537 ,\n",
      "          30.521849  ,  -37.889626  ],\n",
      "       [  95.22371   ,  197.10358   ,   21.453423  ,   33.138184  ,\n",
      "         -91.3965    ,   -1.6697006 ,   48.226017  , -108.11131   ,\n",
      "          22.818317  ,   77.27644   ],\n",
      "       [  46.209396  ,   17.735714  ,  -38.143787  ,   22.876942  ,\n",
      "         -73.498     ,   26.69917   ,   42.845165  ,   -4.606655  ,\n",
      "         -11.495332  ,    5.642314  ],\n",
      "       [  32.32731   ,  141.53775   ,  -64.23381   ,    7.97164   ,\n",
      "         -39.19815   ,   61.637524  ,   40.01863   , -147.85638   ,\n",
      "         -22.069044  ,   51.819225  ],\n",
      "       [ 107.195694  ,   59.10534   ,   55.822372  ,   -3.1747923 ,\n",
      "         -85.96738   ,   23.873898  ,  -41.439056  , -111.43588   ,\n",
      "         -27.494167  ,   56.691196  ],\n",
      "       [  14.858396  ,  -15.746067  ,   -1.6181183 ,  -81.50989   ,\n",
      "           1.7595758 ,   86.419464  , -118.613365  , -119.786575  ,\n",
      "           8.458973  ,  -54.489952  ],\n",
      "       [  68.89317   ,   -5.097273  ,    8.811802  ,   -3.2355022 ,\n",
      "        -113.75881   ,   28.01316   ,   12.196131  ,  -29.831564  ,\n",
      "         -33.80815   ,    0.34610748],\n",
      "       [  65.13188   ,   39.239567  ,  -52.829044  ,  -17.664284  ,\n",
      "         -15.570488  ,   57.712303  ,  -11.356604  , -117.48575   ,\n",
      "         -37.7851    ,   58.742435  ],\n",
      "       [ 179.53789   ,  152.9896    ,  -41.536907  , -173.8988    ,\n",
      "        -141.38182   ,  115.82469   ,   28.771904  ,    0.514061  ,\n",
      "         -44.517258  ,  152.46188   ],\n",
      "       [ 121.26493   ,  150.04068   ,  116.04086   ,  -26.340302  ,\n",
      "        -168.84164   ,  183.18245   ,   21.2554    ,  -83.811584  ,\n",
      "         -62.013866  ,  -21.024973  ],\n",
      "       [  61.2767    ,   28.859652  ,  -44.503212  ,  -25.751724  ,\n",
      "         -22.92778   ,   63.271168  ,  -67.61974   ,  -83.858345  ,\n",
      "           7.4037266 ,   71.079605  ],\n",
      "       [  61.93302   ,  -21.063799  ,  -72.886635  ,  -46.461628  ,\n",
      "         -52.303394  ,   38.491573  ,  -57.776398  ,  -65.44181   ,\n",
      "         -29.099531  ,   11.231719  ],\n",
      "       [  17.781132  ,   16.539177  ,   47.806786  ,  -18.723955  ,\n",
      "         -49.761444  ,  136.31174   ,   13.5568695 , -140.49199   ,\n",
      "          -4.4914093 ,    0.9842956 ],\n",
      "       [  41.955788  ,   21.666855  ,   59.43221   ,  -76.511826  ,\n",
      "         -72.510925  ,   61.12279   , -117.44246   , -110.03589   ,\n",
      "         -39.008205  ,   18.708435  ],\n",
      "       [  37.569393  ,   39.94857   ,   22.186924  ,   25.231594  ,\n",
      "         -11.303387  ,   45.427322  ,  -11.447958  ,   29.542168  ,\n",
      "          26.687408  ,  -41.071907  ],\n",
      "       [  79.909355  ,  126.6944    ,   41.90334   ,  -25.85498   ,\n",
      "        -133.41057   ,   32.73693   ,   -5.789444  ,  -98.535034  ,\n",
      "         -37.37251   ,  -26.293476  ],\n",
      "       [ 120.27393   ,   39.122284  ,  137.96712   ,   50.609413  ,\n",
      "        -131.61176   ,   12.026989  ,   78.23692   , -111.75758   ,\n",
      "         -32.443687  ,  -76.357925  ],\n",
      "       [  18.014309  ,  -23.185825  ,   -4.6567764 ,   35.40206   ,\n",
      "           1.6618366 ,   77.98479   ,  -83.86986   ,  -78.49914   ,\n",
      "          28.019001  ,   -9.957073  ],\n",
      "       [ -24.437302  ,   21.311459  ,  -61.2084    ,  -63.12914   ,\n",
      "          35.698868  ,  100.49983   ,  -10.440374  ,    2.2848778 ,\n",
      "         -19.700405  ,   30.748013  ],\n",
      "       [   4.1715126 ,   17.476788  ,   12.912262  ,  -28.6914    ,\n",
      "         -32.10283   ,   99.01076   , -148.20798   , -116.05668   ,\n",
      "         -19.984215  ,  -30.610023  ]], dtype=float32)\n",
      "  tangent = Traced<ShapedArray(float32[32,10])>with<JaxprTrace(level=1/0)> with\n",
      "    pval = (ShapedArray(float32[32,10]), None)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7f65ec5ba550>, in_tracers=(Traced<ShapedArray(float32[32,10]):JaxprTrace(level=1/0)>, Traced<ShapedArray(float32[10]):JaxprTrace(level=1/0)>), out_tracer_refs=[<weakref at 0x7f65d870b380; to 'JaxprTracer' at 0x7f65d870b330>], out_avals=[ShapedArray(float32[32,10])], primitive=pjit, params={'jaxpr': { \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[32,10]\u001b[39m b\u001b[35m:f32[10]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
      "    \u001b[39m\u001b[22m\u001b[22mc\u001b[35m:f32[1,10]\u001b[39m = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 10)] b\n",
      "    d\u001b[35m:f32[32,10]\u001b[39m = add a c\n",
      "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(d,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'fn', 'keep_unused': False, 'inline': True}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7f65d8724670>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument `optimizer_variables` must be a list of tensors corresponding 1:1 to Adam().variables. Received list with length 6, but expected 14 variables.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m step, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataset):\n\u001b[1;32m     10\u001b[0m     data \u001b[39m=\u001b[39m (data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy(), data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m---> 11\u001b[0m     loss, state \u001b[39m=\u001b[39m train_step(state, data)\n\u001b[1;32m     12\u001b[0m     \u001b[39m# Log every 100 batches.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(state, data)\u001b[0m\n\u001b[1;32m      3\u001b[0m x, y \u001b[39m=\u001b[39m data\n\u001b[1;32m      4\u001b[0m (loss, non_trainable_variables), grads \u001b[39m=\u001b[39m grad_fn(\n\u001b[1;32m      5\u001b[0m     trainable_variables, non_trainable_variables, x, y\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m trainable_variables, optimizer_variables \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39;49mstateless_apply(\n\u001b[1;32m      8\u001b[0m     grads, trainable_variables, optimizer_variables\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[39m# Return updated state\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mreturn\u001b[39;00m loss, (\n\u001b[1;32m     12\u001b[0m     trainable_variables,\n\u001b[1;32m     13\u001b[0m     non_trainable_variables,\n\u001b[1;32m     14\u001b[0m     optimizer_variables,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_core/src/optimizers/base_optimizer.py:289\u001b[0m, in \u001b[0;36mBaseOptimizer.stateless_apply\u001b[0;34m(self, optimizer_variables, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    284\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTo call `stateless_apply`, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmust be built (i.e. its variables must have been created). \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can build it via `optimizer.build(trainable_variables)`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(optimizer_variables) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariables):\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    290\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `optimizer_variables` must be a list of tensors \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcorresponding 1:1 to \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m().variables. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived list with length \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(optimizer_variables)\u001b[39m}\u001b[39;00m\u001b[39m, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariables)\u001b[39m}\u001b[39;00m\u001b[39m variables.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m     )\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trainable_variables) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainable_variables):\n\u001b[1;32m    296\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    297\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `optimizer_variables` must be a list of tensors \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorresponding 1:1 to the trainable variables list that \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainable_variables)\u001b[39m}\u001b[39;00m\u001b[39m variables.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `optimizer_variables` must be a list of tensors corresponding 1:1 to Adam().variables. Received list with length 6, but expected 14 variables."
     ]
    }
   ],
   "source": [
    "optimizer.build(model.trainable_variables)\n",
    "\n",
    "trainable_variables = model.trainable_variables\n",
    "non_trainable_variables = model.non_trainable_variables\n",
    "optimizer_variables = optimizer.variables\n",
    "state = trainable_variables, non_trainable_variables, optimizer_variables\n",
    "\n",
    "# Training loop\n",
    "for step, data in enumerate(train_dataset):\n",
    "    data = (data[0].numpy(), data[1].numpy())\n",
    "    loss, state = train_step(state, data)\n",
    "    # Log every 100 batches.\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Training loss (for 1 batch) at step {step}: {float(loss):.4f}\")\n",
    "        print(f\"Seen so far: {(step + 1) * batch_size} samples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
